{\rtf1\ansi\ansicpg1252\cocoartf2820
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\csgray\c0;}
\margl1440\margr1440\vieww29200\viewh17240\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs21\fsmilli10560 \cf2 This code aims to classify the MNIST dataset using TensorFlow's ResNet50 model and\
contains the complete process from data preprocessing to model training and visualisation.\
First, the code loads the MNIST dataset and extracts the data (images and labels) from\
the training and test sets respectively.The original images of the MNIST dataset are in\
greyscale format with dimensions of `28x28` and pixel values ranging from `[0, 255]`. In\
order to fit the input requirements of ResNet50, the code preprocesses the data as follows:\
normalising the pixel values to `[0, 1]`, converting the single-channel grayscale image to a\
three-channel RGB image using `tf.image.grayscale_to_rgb`, and then resizing the image to\
`[32x32]` with `tf.image.resize` to match the ResNet50 input requirements. 32x32` to match\
the input shape of ResNet50. In addition, the code uses `tf.keras.utils.to_categorical` to One-\
Hot encode the training and test labels into the form of unique hot vectors of categories.\
These preprocessing steps ensure data compatibility with the model.\
Next, the code defines a neural network model based on ResNet50. The backbone\
network is ResNet50, created through the `tf.keras.applications.ResNet50` method, and\
`include_top=False` is set to remove the categorical header of the original model, leaving\
only the feature extraction part. The code does not use ImageNet's pre-training weights\
(`weights=None`) in order to train the MNIST dataset from scratch. A global average\
pooling layer (`GlobalAveragePooling2D`) is added on top of ResNet50 to compress the\
high-dimensional feature mapping into fixed-length vectors, followed by a fully-connected\
layer (`Dense(10)`) that outputs 10 categories corresponding to the numbers 0-9, with an\
activation function of `softmax`.\
The model is compiled with the Adam optimiser chosen and the learning rate set to\
`0.0001`, combined with `categorical_crossentropy` as a loss function to monitor both\
training and validation accuracy. During training, the code saves the model with the highest\
validation accuracy to the file `best_model.h5` via the `ModelCheckpoint` callback function\
for use in subsequent evaluations. Model training is configured to run for 20 epochs with a\
batch size of 128 while using 20% of the training data as the validation set.\
Upon completion of training, the code records and outputs the total training time in\
seconds and plots the accuracy and loss curves for the training and validation sets through\
the function `show_train_history`, denoting the change in accuracy with `\'91accuracy\'92` and `\
\'91val_accuracy\'92` and the change in loss with `\'91loss\'92` and `\'91val_loss\'92`, respectively. loss\'91` and `\'92\
val_accuracy\'91` to denote changes in accuracy, and `\'92loss\'91` and `\'92val_loss"` to denote changes\
in loss. With these curves, it is possible to visualise whether the model converges during\
training, overfitting (validation loss is significantly higher than training loss) or underfitting\
(both have lower accuracy).\
This code can be used to obtain the accuracy and loss of a test set by loading a saved\
best_model.h5 model and using the model.evaluate method to make predictions on the test\
data. In addition, the prediction results can be used to generate classification reports\
(including precision, recall and F1 scores) as well as confusion matrices, and the confusion\
matrices can be visualised by heatmaps to analyse the performance of the model on\
different categories.\
Overall, this code systematically performs data preprocessing, ResNet50 model building\
and training, performance monitoring and visualisation of the training process, and provides\
the ability to save the best model. It is designed to be suitable for extension to other smallimage\
 datasets, especially when migration learning or fine-tuning of pre-trained models\
such as ResNet is required.}